{"cells":[{"cell_type":"markdown","source":["<hr>"],"metadata":{"id":"zgGEklg3aT4w"},"id":"zgGEklg3aT4w"},{"cell_type":"markdown","source":["# Lab 10: Natural Language Processing â€“ Text Classification\n","Total Marks: 8 Marks + 2 Marks (individual assessment) = 10 Marks\n"],"metadata":{"id":"l02Q_htwaT40"},"id":"l02Q_htwaT40"},{"cell_type":"markdown","source":["<hr>"],"metadata":{"id":"CDScx3ORaT41"},"id":"CDScx3ORaT41"},{"cell_type":"markdown","source":["In this assignment you will use a provided `musical dataset` and by using natural language processing, build the `classifiers` and `evaluate` the `performance` of a system that assign `positive (1)` or `negative (0)` score by analyzing text based reviews of musical instruments.<br><br> The dataset is a modified `1000 reviews` of a dataset used in *\"Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering by R. He, J. McAuley WWW, 2016 [cseweb.ucsd.edu]\"*, which is attached with this assignment."],"metadata":{"id":"9AD9yuHKaT41"},"id":"9AD9yuHKaT41"},{"cell_type":"markdown","source":["<b>Accuracy</b> = (TP + TN) / (TP + TN + FP + FN)<br>\n","<b>Precision</b> = TP / (TP + FP)Recall = TP / (TP + FN)<br>\n","<b>F1 Score</b> = 2 * Precision * Recall / (Precision + Recall)<br>"],"metadata":{"id":"lZbMLpGeaT42"},"id":"lZbMLpGeaT42"},{"cell_type":"markdown","source":["## Using Python language, perform the followings NLP tasks to build the classifier for the given dataset:\n"],"metadata":{"id":"-AD3V1XOaT42"},"id":"-AD3V1XOaT42"},{"cell_type":"code","execution_count":null,"source":["#ignore  numpy floating point depreciation warning\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","#text preprocessing\n","import pandas as pd\n","import re \n","import nltk\n","import matplotlib.pyplot as plt\n","import numpy as np\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem.porter import *\n","from nltk.stem.wordnet import WordNetLemmatizer\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","dataset = pd.read_csv('musical1.tsv',sep='\\t')\n","x = dataset['Score'].value_counts()\n","print(\"Class Distribution:\")\n","print(x)\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["Class Distribution:\n","1    533\n","0    467\n","Name: Score, dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /Users/jennylong/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/jennylong/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/jennylong/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"metadata":{"id":"at1Dp_8SaT43","outputId":"52bc5000-bb74-40a3-f637-c63f252e85b6"},"id":"at1Dp_8SaT43"},{"cell_type":"code","execution_count":null,"source":["from bs4 import BeautifulSoup\n","def clean_text_data(data_point, data_size):\n","    review_soup = BeautifulSoup(data_point)\n","    review_text = review_soup.get_text()\n","    #this section removes non-alpha characters\n","    review_letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n","    review_lower_case = review_letters_only.lower()   \n","    \"\"\"\n","    Q1. Using NLTK word_tokenize function, tokenize the given dataset reviews\n","    \"\"\"\n","    review_words = word_tokenize(review_letters_only)\n","    \"\"\"\n","    Q2. Using NLTK PorterStemmer, perform the stemming for the tokens of the reviews\n","    \"\"\"\n","    #stemming\n","    stop_words = stopwords.words(\"english\")\n","    words=[stemmer.stem(word) for word in review_words if word not in stop_words] \n","    \"\"\"\n","    Q3. Using NLTK WordNetLemmatizer, perform the lemmatization for the stemmed tokens\n","    \"\"\"\n","    words = [lemmatizer.lemmatize(word.lower()) for word in words]\n","    return( \" \".join(words)) "],"outputs":[],"metadata":{"id":"vBQjgfpHaT45"},"id":"vBQjgfpHaT45"},{"cell_type":"code","execution_count":null,"source":["training_data_size = dataset[\"Review\"].size\n","\n","for i in range(training_data_size):\n","    dataset[\"Review\"][i] = clean_text_data(dataset[\"Review\"][i], training_data_size)\n","print(\"Cleaning training completed!\")"],"outputs":[{"output_type":"stream","name":"stderr","text":["/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  after removing the cwd from sys.path.\n"]},{"output_type":"stream","name":"stdout","text":["Cleaning training completed!\n"]}],"metadata":{"id":"AkhV6H7EaT45","outputId":"0f366479-7427-4606-d951-8a9b0eaf460d"},"id":"AkhV6H7EaT45"},{"cell_type":"markdown","source":["## 4. Build the Random Forest technique using sklearn library\n"],"metadata":{"id":"QgIRF-8taT46"},"id":"QgIRF-8taT46"},{"cell_type":"code","execution_count":null,"source":["#Getting the features ready to be trained\n","from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer(analyzer = \"word\",   \\\n","                             tokenizer = None,    \\\n","                             preprocessor = None, \\\n","                             stop_words = None,   \\\n","                             max_features = 5000) \n","\n","X_train, X_cv, Y_train, Y_cv = train_test_split(dataset[\"Review\"], dataset[\"Score\"], test_size = 0.2, random_state=1)\n","\n","#converting the train,validation and test data to vectors\n","X_train = vectorizer.fit_transform(X_train)\n","X_train = X_train.toarray()\n","# print(X_train.shape)\n","\n","X_cv = vectorizer.transform(X_cv)\n","X_cv = X_cv.toarray()\n","# print(X_cv.shape)\n","\n","X_test = vectorizer.transform(dataset[\"Review\"])\n","X_test = X_test.toarray()\n","# print(X_test.shape)\n","\n","forest = RandomForestClassifier() \n","forest = forest.fit( X_train, Y_train)"],"outputs":[{"output_type":"stream","name":"stderr","text":["/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"]}],"metadata":{"id":"7p4wtTxkaT47","outputId":"bb680cca-31c7-48e7-a769-d48367a27f19"},"id":"7p4wtTxkaT47"},{"cell_type":"markdown","source":["## Extra Stuff"],"metadata":{"id":"JCSce1IAaT47"},"id":"JCSce1IAaT47"},{"cell_type":"code","execution_count":null,"source":["vocab = vectorizer.get_feature_names()\n","print(f\"Printing first 100 vocabulary samples:\\n{vocab[:100]}\")\n","\n","distribution = np.sum(X_train, axis=0)\n","\n","print(\"Printing first 100 vocab-dist pairs:\")\n","\n","for tag, count in zip(vocab[:100], distribution[:100]):\n","    print(count, tag)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Printing first 100 vocabulary samples:\n","['abcd', 'abil', 'abl', 'ableto', 'ableton', 'abnorm', 'abov', 'abovec', 'abram', 'absolut', 'absolutley', 'absorb', 'abu', 'ac', 'accept', 'access', 'accid', 'acclim', 'accommod', 'accord', 'accordingli', 'account', 'accoust', 'accur', 'accuraci', 'achiev', 'acknowledg', 'acoust', 'acquir', 'acquisit', 'across', 'act', 'action', 'activ', 'actual', 'ad', 'adapt', 'adaptor', 'adario', 'add', 'addario', 'addit', 'address', 'addrio', 'addtion', 'adequ', 'adh', 'adher', 'adjust', 'admit', 'admittedli', 'adult', 'advanc', 'adver', 'adverti', 'advi', 'advic', 'aesthet', 'affect', 'affili', 'affin', 'afford', 'afraid', 'after', 'agc', 'age', 'aggress', 'aglaesel', 'ago', 'agr', 'ahead', 'aid', 'air', 'airi', 'airlin', 'akai', 'akg', 'akustom', 'album', 'alchemi', 'alesi', 'align', 'alittl', 'aliv', 'alkalin', 'allegedli', 'allow', 'allpart', 'almost', 'alnico', 'alon', 'along', 'alot', 'alreadi', 'alright', 'also', 'altanta', 'altern', 'although', 'altogeth']\n","Printing first 100 vocab-dist pairs:\n","4 abcd\n","6 abil\n","21 abl\n","1 ableto\n","5 ableton\n","1 abnorm\n","1 abov\n","1 abovec\n","1 abram\n","13 absolut\n","1 absolutley\n","2 absorb\n","9 abu\n","9 ac\n","9 accept\n","6 access\n","4 accid\n","1 acclim\n","3 accommod\n","2 accord\n","1 accordingli\n","1 account\n","2 accoust\n","3 accur\n","1 accuraci\n","5 achiev\n","3 acknowledg\n","54 acoust\n","1 acquir\n","1 acquisit\n","10 across\n","2 act\n","22 action\n","7 activ\n","43 actual\n","16 ad\n","32 adapt\n","1 adaptor\n","1 adario\n","23 add\n","15 addario\n","9 addit\n","2 address\n","1 addrio\n","1 addtion\n","2 adequ\n","4 adh\n","2 adher\n","48 adjust\n","1 admit\n","4 admittedli\n","2 adult\n","2 advanc\n","1 adver\n","5 adverti\n","6 advi\n","7 advic\n","2 aesthet\n","10 affect\n","2 affili\n","1 affin\n","19 afford\n","3 afraid\n","1 after\n","1 agc\n","4 age\n","4 aggress\n","1 aglaesel\n","24 ago\n","3 agr\n","1 ahead\n","1 aid\n","2 air\n","1 airi\n","1 airlin\n","6 akai\n","5 akg\n","1 akustom\n","5 album\n","1 alchemi\n","1 alesi\n","3 align\n","1 alittl\n","2 aliv\n","1 alkalin\n","1 allegedli\n","17 allow\n","1 allpart\n","46 almost\n","1 alnico\n","10 alon\n","17 along\n","6 alot\n","19 alreadi\n","2 alright\n","119 also\n","1 altanta\n","7 altern\n","17 although\n","2 altogeth\n"]}],"metadata":{"id":"9bcIyHTjaT47","outputId":"0020bc71-0c70-4781-a5df-cfa880beb4ea"},"id":"9bcIyHTjaT47"},{"cell_type":"markdown","source":["## 5. Evaluate the model by finding its accuracy, precision and F1-score"],"metadata":{"id":"1mlAArjzaT48"},"id":"1mlAArjzaT48"},{"cell_type":"code","execution_count":null,"source":["from sklearn.metrics import precision_score,f1_score\n","\n","predictions = forest.predict(X_cv) \n","print(\"Accuracy: \", accuracy_score(Y_cv, predictions))\n","\n","print(\"Precision: %.3f\" %precision_score(Y_cv,predictions))\n","\n","print(\"f1-score: %.3f\" %f1_score(Y_cv,predictions))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.695\n","Precision: 0.724\n","f1-score: 0.714\n"]}],"metadata":{"id":"q6fd2fgtaT48","outputId":"f4dee08f-8126-41d9-9529-30420b3702a2"},"id":"q6fd2fgtaT48"},{"cell_type":"code","execution_count":null,"source":["result = forest.predict(X_test) \n","output = pd.DataFrame( data={\"Score\":result} )"],"outputs":[],"metadata":{"id":"h-F5-V0oaT48"},"id":"h-F5-V0oaT48"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.7.3 64-bit ('base': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"interpreter":{"hash":"398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}